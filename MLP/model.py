import torch.nn as nn

# class BalancedClassifier(nn.Module):
#     def __init__(self, input_dim, num_classes): 
#         super().__init__()
#         self.net = nn.Sequential(
#             # ç¬¬ä¸€å±‚
#             nn.Linear(input_dim, 256),
#             nn.BatchNorm1d(256),  # æ·»åŠ Batch Normalization
#             nn.ReLU(),
#             nn.Dropout(0.4),  # é€‚ä¸­çš„dropout

#             # ç¬¬äºŒå±‚
#             nn.Linear(256, 128),
#             nn.BatchNorm1d(128),  # æ·»åŠ Batch Normalization
#             nn.ReLU(),
#             nn.Dropout(0.3),

#             # ç¬¬ä¸‰å±‚
#             nn.Linear(128, 64),
#             nn.BatchNorm1d(64),  # æ·»åŠ Batch Normalization
#             nn.ReLU(),
#             nn.Dropout(0.2), 
            
#             #ç¬¬å››å±‚
#             nn.Linear(64, 32),
#             nn.BatchNorm1d(32),  # æ·»åŠ Batch Normalization
#             nn.ReLU(),
#             nn.Dropout(0.1),

#             # è¾“å‡ºå±‚
#             nn.Linear(32, num_classes)
#         )

#     def forward(self, x):
#         return self.net(x)


class BalancedClassifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.BatchNorm1d(32),   # ðŸ”¹ BatchNorm1d for stability
            nn.SiLU(),
            nn.Dropout(0.3),

            nn.Linear(32, 16),
            nn.BatchNorm1d(16),   # ðŸ”¹ BatchNorm1d here too
            nn.SiLU(),
            nn.Dropout(0.1),

            nn.Linear(16, num_classes)
        )

    def forward(self, x):
        return self.net(x)
